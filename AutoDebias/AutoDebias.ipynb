{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "XenJIAq1kiTd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-25 14:27:55.254136: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/melaniegalea/opt/anaconda3/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "suRUDi0BklXq"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AdamW, get_scheduler\n",
    "from transformers import BertTokenizer,BertForPreTraining\n",
    "from transformers import RobertaTokenizer,RobertaForMaskedLM,RobertaModel\n",
    "from transformers import AlbertTokenizer, AlbertForPreTraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for BERTu\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "N-YYHE9mlB8o"
   },
   "outputs": [],
   "source": [
    "def get_tokenized_prompt(prompts,tar1_words,tar2_words,tokenizer):\n",
    "    tar1_sen = []\n",
    "    tar2_sen = []\n",
    "    for i in range(len(prompts)):\n",
    "        for j in range(len(tar1_words)):\n",
    "            tar1_sen.append(tar1_words[j]+\" \"+prompts[i]+\" \"+tokenizer.mask_token+\".\")\n",
    "            tar2_sen.append(tar2_words[j]+\" \"+prompts[i]+\" \"+tokenizer.mask_token+\".\")\n",
    "    tar1_tokenized = tokenizer(tar1_sen,padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    tar2_tokenized = tokenizer(tar2_sen,padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    tar1_mask_index = np.where(tar1_tokenized['input_ids'].numpy()==tokenizer.mask_token_id)[1]\n",
    "    tar2_mask_index = np.where(tar2_tokenized['input_ids'].numpy()==tokenizer.mask_token_id)[1]\n",
    "    print(tar1_tokenized['input_ids'].shape)\n",
    "    return tar1_tokenized,tar2_tokenized, tar1_mask_index, tar2_mask_index\n",
    "\n",
    "def send_to_cuda(tar1_tokenized,tar2_tokenized):\n",
    "    for key in tar1_tokenized.keys():\n",
    "        tar1_tokenized[key] = tar1_tokenized[key]\n",
    "        tar2_tokenized[key] = tar2_tokenized[key]\n",
    "    return tar1_tokenized,tar2_tokenized\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hvyH8nX6kB1K",
    "outputId": "3a6183d4-c2f5-40fd-8d4f-860f5d34f2ef"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([39000, 16])\n",
      "epoch 1\n",
      "jsd loss 0.1339079737663269\n",
      "jsd loss 0.006138528697192669\n",
      "jsd loss 0.0029954707715660334\n",
      "jsd loss 0.000837605562992394\n",
      "jsd loss 0.00011619582073763013\n",
      "jsd loss 3.407946496736258e-05\n",
      "jsd loss 7.661541530978866e-06\n",
      "jsd loss 5.025865903007798e-06\n",
      "jsd loss 1.8239347809867468e-06\n",
      "jsd loss 9.956752364814747e-07\n",
      "jsd loss 4.0036070458882023e-07\n",
      "jsd loss 2.9368737841650727e-07\n",
      "jsd loss 3.51465502035353e-07\n",
      "jsd loss 1.4263001446579437e-07\n",
      "jsd loss 1.6196119645428553e-07\n",
      "jsd loss 1.2975758068023424e-07\n",
      "jsd loss 1.1353292705962303e-07\n",
      "jsd loss 8.001372009402985e-08\n",
      "jsd loss 7.839345528282138e-08\n",
      "jsd loss 8.294920661455762e-08\n",
      "jsd loss 7.781058286582265e-08\n",
      "jsd loss 8.731964129538028e-08\n",
      "jsd loss 9.391393973601225e-08\n",
      "jsd loss 9.223401775670936e-08\n",
      "jsd loss 6.312238554073701e-08\n",
      "jsd loss 7.699036075337062e-08\n",
      "jsd loss 8.45476009203594e-08\n",
      "jsd loss 4.5590034858378203e-08\n",
      "jsd loss 7.130148560463567e-08\n",
      "jsd loss 8.040711918511079e-08\n",
      "jsd loss 5.4659338388773904e-08\n",
      "jsd loss 9.850910487330111e-08\n",
      "jsd loss 8.563073805589738e-08\n",
      "jsd loss 6.134493446552369e-08\n",
      "jsd loss 4.7156191840258543e-08\n",
      "jsd loss 3.648043644943755e-08\n",
      "jsd loss 4.9851397676548004e-08\n",
      "jsd loss 2.6551401077767878e-08\n",
      "jsd loss 3.173034102132988e-08\n",
      "jsd loss 4.181895718602391e-08\n",
      "jsd loss 3.170097784277459e-08\n",
      "jsd loss 3.720302288456878e-08\n",
      "jsd loss 3.711989648991221e-08\n",
      "jsd loss 5.0189754574603285e-08\n",
      "jsd loss 3.756614930239266e-08\n",
      "jsd loss 3.0179322152434906e-08\n",
      "jsd loss 2.71122448936012e-08\n",
      "jsd loss 3.029902018170105e-08\n",
      "jsd loss 4.1679712126097e-08\n",
      "jsd loss 3.6525342750337586e-08\n",
      "jsd loss 4.8885098635764734e-08\n",
      "jsd loss 3.587781449709837e-08\n",
      "jsd loss 3.1659499910574596e-08\n",
      "jsd loss 3.773902079728941e-08\n",
      "jsd loss 4.105282513933162e-08\n",
      "jsd loss 4.546878074052074e-08\n",
      "jsd loss 6.034679245203733e-08\n",
      "jsd loss 4.826551958103664e-08\n",
      "jsd loss 5.783353884680764e-08\n",
      "jsd loss 4.3472109467757036e-08\n",
      "jsd loss 3.130889680846849e-08\n",
      "jsd loss 4.008313680969877e-08\n",
      "jsd loss 5.2115865400992334e-08\n",
      "jsd loss 4.467232983529357e-08\n",
      "jsd loss 4.028309064096902e-08\n",
      "jsd loss 5.8275283265629696e-08\n",
      "jsd loss 4.646312845579814e-08\n",
      "jsd loss 5.3438732550148416e-08\n",
      "jsd loss 5.3991470849723555e-08\n",
      "jsd loss 2.628136286375593e-08\n",
      "jsd loss 6.160115617603878e-08\n",
      "jsd loss 1.6957535819983605e-08\n",
      "jsd loss 2.9859446470936746e-08\n",
      "jsd loss 2.764173423486227e-08\n",
      "jsd loss 4.4543703836552595e-08\n",
      "jsd loss 5.34736770418931e-08\n",
      "jsd loss 4.630404504268881e-08\n",
      "jsd loss 6.663333351752954e-08\n",
      "jsd loss 4.514709672776007e-08\n",
      "jsd loss 7.006371305351422e-08\n",
      "jsd loss 6.756228998483493e-08\n",
      "jsd loss 6.369267424588543e-08\n",
      "jsd loss 7.044468475214671e-08\n",
      "jsd loss 5.4641827063051096e-08\n",
      "jsd loss 6.035671162862855e-08\n",
      "jsd loss 4.806942399682157e-08\n",
      "jsd loss 5.96525282503535e-08\n",
      "jsd loss 7.440737448405343e-08\n",
      "jsd loss 7.708435134645697e-08\n",
      "jsd loss 4.8650608874822865e-08\n",
      "jsd loss 8.048067456911667e-08\n",
      "jsd loss 9.445044213407527e-08\n",
      "jsd loss 1.5470320136046212e-07\n",
      "jsd loss 1.1655414766664762e-07\n",
      "jsd loss 4.888807936254125e-08\n",
      "jsd loss 7.243114907851123e-08\n",
      "jsd loss 1.0729573318712937e-07\n",
      "jsd loss 7.152517866870767e-08\n",
      "jsd loss 5.7366179362361436e-08\n",
      "jsd loss 5.971040195618116e-08\n",
      "jsd loss 8.756653357977484e-08\n",
      "jsd loss 1.341479389793676e-07\n",
      "jsd loss 7.072756602610752e-08\n",
      "jsd loss 1.4447391549765598e-07\n",
      "jsd loss 1.190132579154124e-07\n",
      "jsd loss 2.4928948505476e-07\n",
      "jsd loss 2.506159830772958e-07\n",
      "jsd loss 1.0620895096735694e-07\n",
      "jsd loss 6.374848027235203e-08\n",
      "jsd loss 1.0344724188371401e-07\n",
      "jsd loss 1.6669217473008757e-07\n",
      "jsd loss 9.711806114864885e-08\n",
      "jsd loss 9.024281411029733e-08\n",
      "jsd loss 7.783417288464989e-08\n",
      "jsd loss 9.575104087389263e-08\n",
      "jsd loss 1.8570480619928276e-07\n",
      "jsd loss 2.2463575533038238e-07\n",
      "jsd loss 8.581699972864953e-08\n",
      "jsd loss 1.0963546515085909e-07\n",
      "jsd loss 9.577944126704097e-08\n",
      "jsd loss 1.8041464500129223e-07\n",
      "jsd loss 1.2047033237649885e-07\n",
      "jsd loss 1.0083701340590778e-07\n",
      "jsd loss 1.3758409522779402e-07\n",
      "jsd loss 1.97488859043915e-07\n",
      "jsd loss 2.3670298787692445e-07\n",
      "jsd loss 1.0011175533009009e-07\n",
      "jsd loss 9.808316292492236e-08\n",
      "jsd loss 1.8449884464644128e-07\n",
      "jsd loss 2.7092858090327354e-07\n",
      "jsd loss 1.4918913393557887e-07\n",
      "jsd loss 5.240039513410011e-08\n",
      "jsd loss 1.110466456566428e-07\n",
      "jsd loss 7.998328044322989e-08\n",
      "jsd loss 1.1913920161532587e-07\n",
      "jsd loss 9.489318131272739e-08\n",
      "jsd loss 9.726569061285772e-08\n",
      "jsd loss 1.30174640844416e-07\n",
      "jsd loss 1.6327487628586823e-07\n",
      "jsd loss 1.6222593046677503e-07\n",
      "jsd loss 1.445527431087612e-07\n",
      "jsd loss 1.9670682149808272e-07\n",
      "jsd loss 1.569347318763903e-07\n",
      "jsd loss 1.0467828559512782e-07\n",
      "jsd loss 9.233619380211167e-08\n",
      "jsd loss 1.0202748512710968e-07\n",
      "jsd loss 9.147257884478677e-08\n",
      "jsd loss 1.4694421679450897e-07\n",
      "jsd loss 7.79585178634079e-08\n",
      "jsd loss 7.144952007820393e-08\n",
      "jsd loss 8.97782399533753e-08\n",
      "jsd loss 1.1706634950314765e-07\n",
      "jsd loss 1.0964321006667888e-07\n",
      "jsd loss 7.224615217182873e-08\n",
      "jsd loss 8.106350435355125e-08\n",
      "jsd loss 1.4223448374650616e-07\n",
      "jsd loss 5.434288397054843e-08\n",
      "jsd loss 1.4342768395181338e-07\n",
      "jsd loss 1.1712107550465589e-07\n",
      "jsd loss 1.188445821753703e-07\n",
      "jsd loss 5.919053336356228e-08\n",
      "jsd loss 1.2051960140979645e-07\n",
      "jsd loss 7.746128716235035e-08\n",
      "jsd loss 1.2654899705921707e-07\n",
      "jsd loss 8.76208048339322e-08\n",
      "jsd loss 5.207539288676344e-08\n",
      "jsd loss 8.711870691513468e-08\n",
      "jsd loss 6.138112951248331e-08\n",
      "jsd loss 5.341315301166105e-08\n",
      "jsd loss 6.606286717669718e-08\n",
      "jsd loss 1.1227069762753672e-07\n",
      "jsd loss 7.499697574075981e-08\n",
      "jsd loss 1.3633712114824448e-07\n",
      "jsd loss 1.170695256291765e-07\n",
      "jsd loss 9.414152657427621e-08\n",
      "jsd loss 7.220865683166267e-08\n",
      "jsd loss 1.349971086028745e-07\n",
      "jsd loss 1.1106031649887882e-07\n",
      "jsd loss 8.819320385100582e-08\n",
      "jsd loss 9.614080909159384e-08\n",
      "jsd loss 3.450876562283156e-08\n",
      "jsd loss 6.698948595840193e-08\n",
      "jsd loss 1.418078738879558e-07\n",
      "jsd loss 8.579919352769139e-08\n",
      "jsd loss 5.519846624224556e-08\n",
      "jsd loss 6.274082409163384e-08\n",
      "jsd loss 8.792568451099214e-08\n",
      "jsd loss 6.238697380922531e-08\n",
      "jsd loss 5.6777164303412064e-08\n",
      "jsd loss 9.203617423736432e-08\n",
      "jsd loss 8.118645666854718e-08\n",
      "jsd loss 5.20848004725849e-08\n",
      "jsd loss 6.005620889482088e-08\n",
      "jsd loss 4.7636540045914444e-08\n",
      "jsd loss 6.993481349581998e-08\n",
      "jsd loss 7.800401391477862e-08\n",
      "jsd loss 4.9127997669984325e-08\n",
      "jsd loss 5.3990110160384575e-08\n",
      "jsd loss 9.123196775817632e-08\n",
      "jsd loss 7.836317195142328e-08\n",
      "jsd loss 8.967647602275974e-08\n",
      "jsd loss 8.852138222437134e-08\n",
      "jsd loss 6.839339050657145e-08\n",
      "jsd loss 9.854434068756746e-08\n",
      "jsd loss 7.686339387191765e-08\n",
      "jsd loss 5.826786519946836e-08\n",
      "jsd loss 7.626621112422072e-08\n",
      "jsd loss 1.1057309023954076e-07\n",
      "jsd loss 1.0891665169765474e-07\n",
      "jsd loss 6.448625811117381e-08\n",
      "jsd loss 1.0539366712691844e-07\n",
      "jsd loss 5.832713156905811e-08\n",
      "jsd loss 7.494247000749965e-08\n",
      "jsd loss 4.14223357836363e-08\n",
      "jsd loss 3.494790945524073e-08\n",
      "jsd loss 7.485965625164681e-08\n",
      "jsd loss 4.6664197839163535e-08\n",
      "jsd loss 6.267703867024466e-08\n",
      "jsd loss 2.9009678215174972e-08\n",
      "jsd loss 6.343813652165409e-08\n",
      "jsd loss 3.8661177370613586e-08\n",
      "jsd loss 1.1692330303958443e-07\n",
      "jsd loss 5.355337862056331e-08\n",
      "jsd loss 3.922835745129305e-08\n",
      "jsd loss 5.009400538824593e-08\n",
      "jsd loss 4.306215117821921e-08\n",
      "jsd loss 9.785206600554375e-08\n",
      "jsd loss 3.602987419526471e-08\n",
      "jsd loss 3.7141258957262835e-08\n",
      "jsd loss 6.1553492969324e-08\n",
      "jsd loss 4.24653165964628e-08\n",
      "jsd loss 3.67551322710824e-08\n",
      "jsd loss 8.93797249545969e-08\n",
      "jsd loss 5.728361784917979e-08\n",
      "jsd loss 5.4859793152672864e-08\n",
      "jsd loss 1.0691385909922246e-07\n",
      "jsd loss 7.630116272139276e-08\n",
      "jsd loss 8.078926327925728e-08\n",
      "jsd loss 4.299388933759474e-08\n",
      "jsd loss 5.258293711563056e-08\n",
      "jsd loss 6.769040794551984e-08\n",
      "jsd loss 5.4401503746248636e-08\n",
      "jsd loss 1.2420666450907447e-07\n",
      "jsd loss 7.811982527528016e-08\n",
      "jsd loss 3.803311443562052e-08\n",
      "jsd loss 4.6412825582820005e-08\n",
      "jsd loss 3.4924948266734646e-08\n",
      "jsd loss 6.752073744564768e-08\n",
      "jsd loss 8.377155324978958e-08\n",
      "jsd loss 7.646426780638649e-08\n",
      "jsd loss 3.631645384416515e-08\n",
      "jsd loss 3.830159300832747e-08\n",
      "jsd loss 4.9977483485008634e-08\n",
      "jsd loss 5.565237159999015e-08\n",
      "jsd loss 5.31017221305774e-08\n",
      "jsd loss 4.657896823800911e-08\n",
      "jsd loss 5.377550138518927e-08\n",
      "jsd loss 1.0813042194968148e-07\n",
      "jsd loss 1.0836906483291386e-07\n",
      "jsd loss 4.0234745313227904e-08\n",
      "jsd loss 3.695788564073155e-08\n",
      "jsd loss 3.614349708414011e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jsd loss 7.797980572377128e-08\n",
      "jsd loss 4.04648901053406e-08\n",
      "jsd loss 3.5382527130423114e-08\n",
      "jsd loss 3.121440528275343e-08\n",
      "jsd loss 6.136325225725159e-08\n",
      "jsd loss 6.50346976271976e-08\n"
     ]
    }
   ],
   "source": [
    "model_type = 'bert'  # Replace with the desired model type\n",
    "model_name_or_path = 'bert-base-uncased'  # Replace with the desired model path\n",
    "data_path = 'data_mt/'  # Replace with the desired data path\n",
    "prompts_file = 'prompts_bert-base-uncased_gender'  # Replace with the desired prompts file\n",
    "debias_type = 'gender'  # Replace with the desired debias type\n",
    "finetuning_vocab_file = None  # Replace with the desired finetuning vocab file\n",
    "batch_size = 32  # Replace with the desired batch size\n",
    "lr = 0.001  # Replace with the desired learning rate\n",
    "epochs = 1  # Replace with the desired number of epochs\n",
    "tune_pooling_layer = True  # Replace with True or False based on your requirement\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "    if model_type == 'bert':\n",
    "#         tokenizer = BertTokenizer.from_pretrained(model_name_or_path)\n",
    "#         model = BertForPreTraining.from_pretrained(model_name_or_path)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"MLRS/BERTu\")\n",
    "        model = AutoModelForMaskedLM.from_pretrained(\"MLRS/BERTu\")\n",
    "    elif model_type == 'roberta':\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(model_name_or_path)\n",
    "        model = RobertaForMaskedLM.from_pretrained(model_name_or_path)\n",
    "        new_roberta= RobertaModel.from_pretrained(model_name_or_path) #make the add_pooling_layer=True\n",
    "        model.roberta = new_roberta\n",
    "    elif model_type == 'albert':\n",
    "        tokenizer = AlbertTokenizer.from_pretrained(model_name_or_path)\n",
    "        model = AlbertForPreTraining.from_pretrained(model_name_or_path)\n",
    "    else:\n",
    "        raise NotImplementedError(\"not implemented!\")\n",
    "    model.train()\n",
    "\n",
    "    searched_prompts = load_word_list(data_path + prompts_file)\n",
    "    if debias_type == 'gender':\n",
    "        male_words_ = load_word_list(data_path+\"male.txt\")\n",
    "        female_words_ = load_word_list(data_path+\"female.txt\")\n",
    "        tar1_words, tar2_words = clean_word_list2(male_words_, female_words_,tokenizer)   #remove the OOV words\n",
    "        tar1_tokenized,tar2_tokenized,tar1_mask_index,tar2_mask_index = get_tokenized_prompt(searched_prompts, tar1_words, tar2_words, tokenizer)\n",
    "        tar1_tokenized,tar2_tokenized =send_to_cuda(tar1_tokenized,tar2_tokenized)\n",
    "    elif debias_type=='race':\n",
    "        race1_words_ = load_word_list(data_path+\"race1.txt\")\n",
    "        race2_words_ = load_word_list(data_path+\"race2.txt\")\n",
    "        tar1_words, tar2_words = clean_word_list2(race1_words_, race2_words_,tokenizer)\n",
    "        tar1_tokenized,tar2_tokenized,tar1_mask_index,tar2_mask_index = get_tokenized_prompt(searched_prompts, tar1_words, tar2_words, tokenizer)\n",
    "        #tar1_tokenized,tar2_tokenized =send_to_cuda(tar1_tokenized,tar2_tokenized)\n",
    "\n",
    "    if finetuning_vocab_file:\n",
    "        finetuning_vocab_ = load_word_list(data_path+finetuning_vocab_file)\n",
    "        finetuning_vocab = tokenizer.convert_tokens_to_ids(finetuning_vocab_)\n",
    "\n",
    "    jsd_model = JSD()\n",
    "\n",
    "    assert tar1_tokenized['input_ids'].shape[0] == tar2_tokenized['input_ids'].shape[0]\n",
    "    data_len = tar1_tokenized['input_ids'].shape[0]\n",
    "\n",
    "    idx_ds = DataLoader([i for i in range(data_len)], batch_size = batch_size, shuffle=True,drop_last=True)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    for i in range(1,epochs+1):\n",
    "        print(\"epoch\",i)\n",
    "\n",
    "        # load data\n",
    "        for idx in idx_ds:\n",
    "            tar1_inputs={}\n",
    "            tar2_inputs={}\n",
    "            for key in tar1_tokenized.keys():\n",
    "                tar1_inputs[key]=tar1_tokenized[key][idx]\n",
    "                tar2_inputs[key]=tar2_tokenized[key][idx]\n",
    "            tar1_mask = tar1_mask_index[idx]\n",
    "            tar2_mask = tar2_mask_index[idx]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            tar1_predictions = model(**tar1_inputs)\n",
    "            tar2_predictions = model(**tar2_inputs)\n",
    "#             print('tar1_predictions: ', tar1_predictions)\n",
    "#             print('tar2_predictions: ', tar2_predictions)\n",
    "\n",
    "            if finetuning_vocab_file:\n",
    "                tar1_predictions_logits = tar1_predictions.prediction_logits[torch.arange(tar1_predictions.logits.size(0)), tar1_mask][:,finetuning_vocab]\n",
    "                tar2_predictions_logits = tar2_predictions.prediction_logits[torch.arange(tar2_predictions.logits.size(0)), tar2_mask][:, finetuning_vocab]\n",
    "            else:\n",
    "#                 tar1_predictions_logits = tar1_predictions.prediction_logits[torch.arange(tar1_predictions.prediction_logits.size(0)), tar1_mask]\n",
    "#                 tar2_predictions_logits = tar2_predictions.prediction_logits[torch.arange(tar2_predictions.prediction_logits.size(0)), tar2_mask]\n",
    "                tar1_predictions_logits = tar1_predictions.logits[torch.arange(tar1_predictions.logits.size(0)), tar1_mask]\n",
    "                tar2_predictions_logits = tar2_predictions.logits[torch.arange(tar2_predictions.logits.size(0)), tar2_mask]\n",
    "\n",
    "            jsd_loss = jsd_model(tar1_predictions_logits,tar2_predictions_logits)\n",
    "            loss =jsd_loss\n",
    "\n",
    "            if tune_pooling_layer:\n",
    "                if model_type == 'bert':\n",
    "                    #----------- FOR BERTu: ------------------\n",
    "                    # Assuming tar1_inputs contains your input data\n",
    "                    tar1_outputs = model.bert(**tar1_inputs)\n",
    "                    # Get the last hidden states (output of the last layer)\n",
    "                    last_hidden_states = tar1_outputs.last_hidden_state\n",
    "                    # Extract the [CLS] token representation (first token)\n",
    "                    cls_token_representation = last_hidden_states[:, 0, :]\n",
    "                    # Now, cls_token_representation contains the representation of [CLS] token\n",
    "                    tar1_embedding = cls_token_representation\n",
    "                    \n",
    "                    #Assuming tar2_inputs contains your input data\n",
    "                    tar2_outputs = model.bert(**tar2_inputs)\n",
    "                    # Get the last hidden states (output of the last layer)\n",
    "                    last_hidden_states = tar2_outputs.last_hidden_state\n",
    "                    # Extract the [CLS] token representation (first token)\n",
    "                    cls_token_representation = last_hidden_states[:, 0, :]\n",
    "                    # Now, cls_token_representation contains the representation of [CLS] token\n",
    "                    tar2_embedding = cls_token_representation\n",
    "                    #-----------------------------------------\n",
    "\n",
    "                    #----------- FOR BERT: ------------------\n",
    "                    #tar1_embedding = model.bert(**tar1_inputs).pooler_output\n",
    "                    #tar2_embedding = model.bert(**tar2_inputs).pooler_output\n",
    "                    #-----------------------------------------\n",
    "                    \n",
    "#                     print('model: ', model)\n",
    "#                     print('tar1_embedding: ', tar1_embedding)\n",
    "                elif model_type == 'roberta':\n",
    "                    tar1_embedding = model.roberta(**tar1_inputs).pooler_output\n",
    "                    tar2_embedding = model.roberta(**tar2_inputs).pooler_output\n",
    "                elif model_type == 'albert':\n",
    "                    tar1_embedding = model.albert(**tar1_inputs).pooler_output\n",
    "                    tar2_embedding = model.albert(**tar2_inputs).pooler_output\n",
    "                embed_dist = 1-F.cosine_similarity(tar1_embedding,tar2_embedding,dim=1)\n",
    "                embed_dist = torch.mean(embed_dist)\n",
    "                loss =jsd_loss+0.1*torch.mean(embed_dist)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            print('jsd loss {}'.format(jsd_loss))\n",
    "        model.save_pretrained('model/debiased_model_{}_{}'.format(model_name_or_path, debias_type))\n",
    "        tokenizer.save_pretrained('model/debiased_model_{}_{}'.format(model_name_or_path, debias_type))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
