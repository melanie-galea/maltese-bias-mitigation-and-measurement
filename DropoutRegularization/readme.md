BERT has two dropout parameters which may be configured, one for attention weights (a) and another for hidden activations (h), both set to 0.1 by default.
Experimentations were done with values 0.1, 0.15 and 0.2.

Install the following package:
datasets
